import os
import sys
import multiprocessing
from time import gmtime, strftime

# Th√™m current directory v√†o Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

def run_single_spider():
    """Ch·∫°y spider ƒë∆°n v·ªõi high concurrency"""
    try:
        from scrapy.crawler import CrawlerProcess
        from scrapy.utils.project import get_project_settings
        from crawl_phone import JobSpider
        
        # L·∫•y settings
        settings = get_project_settings()
        
        # T·ªëi ∆∞u settings cho single process
        settings.set('CONCURRENT_REQUESTS', 32)
        settings.set('CONCURRENT_REQUESTS_PER_DOMAIN', 16)
        settings.set('DOWNLOAD_DELAY', 1)
        settings.set('RANDOMIZE_DOWNLOAD_DELAY', 0.5)
        settings.set('AUTOTHROTTLE_ENABLED', True)
        settings.set('AUTOTHROTTLE_START_DELAY', 0.5)
        settings.set('AUTOTHROTTLE_MAX_DELAY', 3)
        settings.set('AUTOTHROTTLE_TARGET_CONCURRENCY', 2.0)
        settings.set('AUTOTHROTTLE_DEBUG', True)
        
        # Memory v√† performance settings
        settings.set('MEMUSAGE_ENABLED', True)
        settings.set('MEMUSAGE_LIMIT_MB', 2048)
        settings.set('REACTOR_THREADPOOL_MAXSIZE', 20)
        
        # Cache v√† retry
        settings.set('HTTPCACHE_ENABLED', True)
        settings.set('RETRY_ENABLED', True)
        settings.set('RETRY_TIMES', 3)
        
        print("üöÄ Starting optimized single spider...")
        print(f"‚öôÔ∏è  Concurrent requests: {settings.get('CONCURRENT_REQUESTS')}")
        print(f"‚öôÔ∏è  Per domain: {settings.get('CONCURRENT_REQUESTS_PER_DOMAIN')}")
        print(f"‚öôÔ∏è  Download delay: {settings.get('DOWNLOAD_DELAY')}")
        print(f"‚öôÔ∏è  AutoThrottle: {settings.get('AUTOTHROTTLE_ENABLED')}")
        
        # T·∫°o v√† ch·∫°y crawler
        process = CrawlerProcess(settings)
        process.crawl(JobSpider)
        process.start()
        
        print("‚úÖ Spider completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error running spider: {e}")
        import traceback
        print(traceback.format_exc())
        return False

def run_spider_in_process(process_id, settings_dict):
    """Ch·∫°y spider trong process ri√™ng bi·ªát"""
    try:
        import os
        print(f"üöÄ Process {process_id} (PID: {os.getpid()}) starting...")
        
        from scrapy.crawler import CrawlerProcess
        from scrapy.settings import Settings
        from crawl_phone import JobSpider
        
        # T·∫°o settings
        settings = Settings(settings_dict)
        
        # Adjust settings cho process n√†y
        settings.set('CONCURRENT_REQUESTS', max(8, settings_dict.get('CONCURRENT_REQUESTS', 32) // 2))
        settings.set('CONCURRENT_REQUESTS_PER_DOMAIN', max(4, settings_dict.get('CONCURRENT_REQUESTS_PER_DOMAIN', 16) // 2))
        
        # T·∫°o v√† ch·∫°y crawler
        process = CrawlerProcess(settings)
        process.crawl(JobSpider)
        process.start()
        
        print(f"‚úÖ Process {process_id} completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Process {process_id} error: {e}")
        import traceback
        print(traceback.format_exc())
        return False

def main():
    """Main function ƒë∆°n gi·∫£n"""
    print("=" * 60)
    print("üï∑Ô∏è Simple Multi-threaded Crawler")
    print("=" * 60)
    print(f"üïê Started at: {strftime('%Y-%m-%d %H:%M:%S', gmtime())}")
    print(f"üíª Available CPU cores: {multiprocessing.cpu_count()}")
    print(f"üêç Python version: {sys.version.split()[0]}")
    print("-" * 60)
    
    print("\nChoose running mode:")
    print("1. Single process with high concurrency (RECOMMENDED)")
    print("2. Exit")
    
    try:
        choice = input("\nEnter your choice (1-2): ").strip()
    except KeyboardInterrupt:
        print("\nExiting...")
        return
    
    if choice == "1":
        print("\n" + "="*50)
        print("üöÄ SINGLE PROCESS MODE (HIGH CONCURRENCY)")
        print("="*50)
        print("This mode uses:")
        print("‚úÖ 32 concurrent requests")
        print("‚úÖ 16 requests per domain")
        print("‚úÖ Auto-throttling enabled")
        print("‚úÖ Memory monitoring")
        print("‚úÖ HTTP caching")
        print("-" * 50)
        
        try:
            success = run_single_spider()
            if success:
                print("\nüéâ Crawling completed successfully!")
            else:
                print("\n‚ùå Crawling failed!")
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è Crawling interrupted by user")
    
    elif choice == "2":
        print("üëã Goodbye!")
        return
    
    else:
        print("‚ùå Invalid choice. Please enter 1, 2")
        return
    
    print(f"\n‚úÖ Session ended at: {strftime('%Y-%m-%d %H:%M:%S', gmtime())}")
    print("=" * 60)

if __name__ == "__main__":
    # Set multiprocessing start method for compatibility
    if hasattr(multiprocessing, 'set_start_method'):
        try:
            multiprocessing.set_start_method('spawn', force=True)
        except RuntimeError:
            pass  # Already set
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã Program terminated by user")
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        print(traceback.format_exc())
    
    print("\nThank you for using Crawler! üï∑Ô∏è")